# Sample Logstash configuration for creating a simple
# csv -> Logstash -> Elasticsearch pipeline.

input {
    file {
        path => "/home/shahzad/Desktop/Work/ELK Stack/es-sample-data/poi.csv"
        start_position => "beginning"
		sincedb_path => "nul"
    }
}

filter { 
	csv { 
		columns => ["id","fkey","name","compound_address_parents","subcat_name","telephone","lng","lat","operation","op_date","geom"] 
		separator => ","
	}
	#Drop header in the csv file
	if [id] == "id" {
  		drop { }
	}
	date {
		#match date column to date type 
		match => [ "op_date", "YYYY-MM-dd'T'HH:mm:ss.SSSZ", "YYYY-MM-dd HH:mm:ss.SSSSSS" ]
		target => "log_date"
		
		#remove_field => ["op_date","telephone","lng","lat","geom"]
	}
	mutate {
	#convert column types
	convert => {
	      "id" => "integer"
	      "fkey" => "integer"
	    }
	#remove fields while indexing csv file
	remove_field => ["op_date","telephone","lng","lat","geom"]
	}
}

output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "pois"
    action => "index"
    #user => "elastic"
    #password => "changeme"
  }
stdout {}
}
